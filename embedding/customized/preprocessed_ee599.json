{
    "0": "questions : welcome to piazza ! students , welcome to piazza ! we 'll be conducting all class-related discussion here this term . the quicker you begin asking questions on piazza ( rather than via emails ) , the quicker you 'll benefit from the collective knowledge of your classmates and instructors . we encourage you to ask questions when you 're struggling to understand a concept—you can even do so anonymously.-arash answer : no answer",
    "1": "questions : office hours professor : murali annavaramoffice : eeb 232office hours : friday 11:00 am-12:30 pmcontact info : annavara @ usc.eduprofessor : arash saifhashemivirtual : wed and thu 5 to 6pm . send me an email if i was not in the zoom meeting . in-person : fridays please email to make an appointmentzoom link- meeting id : 916 8764 5697 passcode : 980225contact info : saifhash @ usc.eduteaching assistant 1 : yongqin wangoffice : phe320office hours : friday 3:00 pm to 5:00 pmcontact info : yongqin @ usc.eduteaching assistant 2 : lei gaooffice : phe320office hours : wednesday 1:00 pm to 3:00 pmcontact info : leig @ usc.eduteaching assistant 3 : chaoyi jiangoffice : phe320office hours : monday 4:00 pm to 6:00 pmcontact info : chaoyij @ usc.edu answer : no answer",
    "2": "questions : lecture calendar last update : nov 28th , 2023 answer : no answer",
    "3": "questions : github related information collection hi class , please use this google sheet to provide your github related information.https : //docs.google.com/spreadsheets/d/1030g1gdfkaz8oytyyz_xkfkgxpv72uica1hp5qa88fw/edit # gid=0 answer : no answer",
    "4": "questions : discussion week1 material posted discussion slides and a pytorch example is posted under piazza resources . answer : no answer",
    "5": "questions : hw1 clarification hi class , the sgd and mini-batch gd algorithms i covered in the class are not exactly the same as described in hw1 . my previous definition of one iteration of sgd or mini-batch gd was going over the entire training set after computing gradients on every single sample or every batch of samples ( which is a common practice ) . instead , hw1 wants you to randomly pick a single sample or a batch of samples to compute gradients on it , then call it an iteration . i have updated the discussion slides for your reference . let ’ s stick to hw1 ’ s definition of sgd and mini-batch gd when you program and analyze the performance of those algorithms . sorry for the confusion caused.also , you can access hw link on piazza under resources.-lei answer : no answer",
    "6": "questions : required reading i have the 2 sources for required reading on the syllabus . is there somewhere to see what sections to read before each class ? thank you answer : https : //web-app.usc.edu/soc/syllabus/20233/30693.pdf `` efficient processing of dnns '' is available in usc libraries online if you want to download it",
    "7": "questions : hw1 ddl extension and discussion 1 video hw1 deadline is extended to september 4th . please come to office hours or ask on piazza if you have any questions.for the last question of hw1 , check the pytorch example on piazza , which will help you understand the basic usage of pytorch to implement the gd algorithm.we have also posted the video record for the first discussion section on piazza . some slides in the discussion video were not updated , so please refer to the current one when doing your hw1 . answer : no answer",
    "8": "questions : course video links dear studentshere are the course video recordings for the first 2 lectures . we will update these links with additional video recordings as feasible.lecture 1 : aug 23 : murali annavaramhttps : //usc.zoom.us/rec/share/jsk_jvixw2x639iebfzgh-_35a2almd-wztd-shv4a-v0dnizncsfbedme9i_4qi.2w6zmcfnduuxc3h4lecture 2 : aug 25 : murali annavaramhttps : //usc.zoom.us/rec/share/q9aq0ox4abwdtxofjsc9e_brp8toxkuxhl3hyjrm4hxxxlsojmuagd8iwjlqep_h.551w_cv5ybvj4p2g answer : no answer",
    "9": "questions : midterm and final date midterm will be on oct 11th and final will be on dec 8th . please check the lecture calendar . answer : no answer",
    "10": "questions : [ solved ] problem in updating parameters and running stochastic gd # # ~~updating parameter problem~~ because of wrong residual computation * * solution : check $ $ \\frac { \\sigma j } { \\sigma \\hat { y_i } } = \\frac { 2 } { n } ( \\hat { y_i } - y_i ) $ $ . i reverse the order of $ $ \\hat { y_i } - y_i $ $ * * i learned that in the logistic regression , we will update the parameter w and b with $ $ w ' = w - \\alpha * dw $ $ . but when i did this with the linear regression coding , i got the opposite result as follows : increasing cost as the iteration increases with $ $ w ' = w - \\alpha * dw $ $ . ! [ image.png ] ( /redirect/s3 ? bucket=uploads & prefix=paste % 2fky842rtdgkpg4 % 2fab3fe2fac6f3e8a9a0db8166bf49e1b7ae21f2cdaae76e2f79cf3c2adab27fc9 % 2fimage.png ) ! [ image.png ] ( /redirect/s3 ? bucket=uploads & prefix=paste % 2fky842rtdgkpg4 % 2ffafe7d4465490b5442fb45ae39490ab6af325a6d28682786fd491450f0a63744 % 2fimage.png ) a mono-decreasing function with $ $ w ' = w + \\alpha * dw $ $ . ! [ image.png ] ( /redirect/s3 ? bucket=uploads & prefix=paste % 2fky842rtdgkpg4 % 2fb1b9e7c82233c37d5fec19e6d855877f07d99602fb3494dc605644722238afe6 % 2fimage.png ) ! [ image.png ] ( /redirect/s3 ? bucket=uploads & prefix=paste % 2fky842rtdgkpg4 % 2f8d562b23d0d1db6c37d8bcaa8b651feb19ba0a7a5880d9f65c84560301b7efbf % 2fimage.png ) # # ~~stochastic gd wo n't run~~because difference on return value * * solution : i returned dictionary instead of tuple for all the functions . when i make it compatible with those functions inside sgd , it works . * * i am not sure what we are going to do with sgd , because i think it may be completed in advanced . but when i tried to run it , it comes with the following error : `` ` start : theta0 : 1.000 , theta1 : 1.000 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - ufunctypeerror traceback ( most recent call last ) in ( ) 10 ) 11 x_b = np.c_ [ np.ones ( ( len ( x ) , 1 ) ) , x ] -- - > 12 theta , cost_history = stochastic_gradient_descent_with_momentum ( 13 x_b , y , theta , learning_rate , momentum , num_iterations 14 ) * * * 2 frames * * * /usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py in dot ( * args , * * kwargs ) ufunctypeerror : ufunc 'multiply ' did not contain a loop with signature matching types ( dtype ( 'none `` ` i have no clue what is wrong with this since no multiply is used in the function and i am not the one finishing this function . btw , it seems that we are not using the sgd without momentom and velocity in the code . how are we going to verify that we are doing correct with all the code ? answer : what is your definition of model residual ? for this hw , we assume it is prediction - y , so the gradient will be 2/n * x^t * residual . if you define model residual as y - prediction , then gradient will be -2/n * x^t * residual . those two gradient equations are identical but be careful with the sign when you code it.for sgd , you are supposed to run it directly . i can see the error is related to multiplying strings , but i am not sure what causes this error . please check the functions you implement inside sgd.you can follow the plot function from other subsections to test your code . as long as you see a faster converging loss curve , you should be fine .",
    "11": "questions : today 's * * lecture * * will be online only ! dear student , today 's lecture at6:30pmwill be on zoom only . no need to come to the class in person.note that this post is about today'slecture , not the discussion session.zoom session info : meeting id : 941 6772 0397zoom linkarash answer : no answer",
    "12": "questions : discussion 2 materials and online tutorial we uploaded discussion 2 materials and an online tutorial about matrix differentiation under resources . please check to enhance your understanding . answer : no answer",
    "13": "questions : confuse about the question in hw1 for these algorithms : gd , sgd , sgd with momentum , mbgd , answer these questions : 1. assuming the learning rate is the same for all algorithms , what is the worst case runtime complexity of each algorithm . 6. assuming the same rating rate and number of iterations , how many operations does each method performs in total ? looks like this two question are very similar . is there any difference i should notice to answer these questions ? answer : the first one is the complexity analysis , and the second one is to show the exact number of operations .",
    "14": "questions : zoom recording for the lecture on 30th aug 2023 where can i find the slides and the zoom recording of the class taken on 30th aug 2023 by prof. arash ? answer : they should be available on blackboard .",
    "15": "questions : lecture2.pdf - arash has been added to class resources page under lecture notes the teaching staff has posted a new lecture notes resource.title : lecture2.pdf - arashhttps : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lm0wcmre9774c9you can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "16": "questions : monday office hours will there still be office hours this monday from 4-6 pm ? answer : there will be no office hours on 9/4 labor day holiday .",
    "17": "questions : hw1 q6 for the question regarding the number of operations in each method , do we assume that matrix multiplication is one operation or multiple operations , as in we need to consider each multiplication and summation within matrix multiplication ? answer : consider each mul and addition as an operation . you can also declare any reasonable assumptions before answering those questions , and then make sure your explanation aligns with your assumptions .",
    "18": "questions : homework 2 posted ! hi all , homework 2 has been posted . it is due september 14th , 2023. you can find the theinvitation linkin resources - > homework.thanks , yongqin wang answer : no answer",
    "19": "questions : homework submission hi class , it seems like some people are confused about how to submit their homework . you do n't need to upload your code to somewhere else . all you need to do is make sure your github repo is up to date . we will grade your latest version of the code.thank you . answer : no answer",
    "20": "questions : hw2 laplacian sharpening 1. for steps 2 and 4 , what does it mean to scale into the range of the original image ? my intuition is that , for the context of this problem , the scale of the filter is not exceeding the range of the original image.2 . the size of the filter at step 3 does not match the size of the original image . do we scale it back to a 10x10 before adding ? answer : sorry for the confusion , the steps should be1 . use the laplacian filter to convolve the original image to obtain a filtered image2 . scaled the filtered image into the range of the original image3 . add the filtered image to the original image4 . scale the sum to the range of the original image againalso , by scale the a vector , whose range is [ x , y ] into a range [ a , b ] , i mean to scale the vector such that the range of the vector is [ x , y ]",
    "21": "questions : hw2 laplacian filter clarification the steps for applying sharpening is:1. use the laplacian filter to convolve the original image to obtain a filtered image2 . scale the filtered image into the range of the original image3 . add the filtered image to the original image4 . scale the sum to the range of the original image againalso , by scale the a vector , whose range is [ x , y ] into a range [ a , b ] , i mean to scale the vector such that the range of the vector is [ x , y ] .sorry for the confusion. -- -- -to further clarify the second and fourth step , scale a matrix x means you need to apply min-max normalization following this equation : answer : no answer",
    "22": "questions : today 's class will be online only ! dear student , i have an unexpected meeting and wo n't be able to get to the campus on time.today 's lecture at6:30pmwill beon zoom only . no need to come to the class in person.note that this post is about today'slecture , not the discussion session.also , friday 's lecture will be in person as usual.zoom session info ( also accessible from blackboard ) : meeting id : 941 6772 0397zoom linkarash answer : no answer",
    "23": "questions : blurring the image with the gaussian filter how much does the image need to be blurred . my new image seems faintly blurred so i 'm not sure if that 's the expected outcome or if there should be significant blurring . answer : faintly blurred should be okay . if you are uncertain , you are welcome to come to office hours to let us take a look .",
    "24": "questions : question 2 input/ouput dims do you want the output dimensions before or after max pooling ? answer : list linear and conv2d should be fine",
    "25": "questions : question about midterm duration on the syllabus it mentions that the midterm will be oct 11 during the evening lecture . so would the duration of the midterm be the full 110 minutes from 6:30-8:20 ? thank you answer : to be determined .",
    "26": "questions : laplacian sharpening final image is grayed out when i do laplacian sharpening with all the scaling , the final image looks grayed out . i think it is because the edges end up with a negative value and then when the image is rescaled , zero values become positive . i 'm not sure if this is wrong . thanks ! answer : you are right . since we ask you to apply laplacian sharpening to the 10x10 sample image , you can also check the pattern of that image .",
    "27": "questions : pixel values when performing edge detection , laplacian sharpening , and gaussian filtering on the car photo , if the input grayscale image has pixel values in the range ( 0 , 255 ) , should the resulting processed image also be transformed to have pixel values in the range ( 0 , 255 ) ? answer : i believe plt.imshow ( ) scales the input automatically . you can just give your filtered image to the plot function . for sharpenning you are asked to manually scale your result before you plot it .",
    "28": "questions : video recording for week-2 hi professor , when will you be uploading the lecture videos for week-2 ? i am unable to find it in the resources tab . it would be helpful to review the week-2 contents before the next lecture . thank you ! answer : these recordings are available in blackboard - > usc zoom pro meeting - > cloud recordings",
    "29": "questions : lecture slides for week 3 hi professor , please could you upload week 3 lecture slides.thank you ! answer : @ 44",
    "30": "questions : week 4 slides are added on the resources page the teaching staff has posted a new lecture notes resource.you can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "31": "questions : plotting cars-1638594_1280.jpg after converting the image to grayscale np array the size of the array is ( 853,1280 ) . i have scaled this to have values ( 0,1 ) . theplot_image_with_values ( ) function is executing for a very long time . is there a faster way to do it ? answer : you can use plt.matshow you can use ` plt.imshow ( image , cmap='gray ' ) ` to plot your image instead of calling the ` plot_image_with_values ( ) ` function , because the function will add pixel values to each cell and slow down your execution .",
    "32": "questions : question 2 : model dimensions does calculating backward pass multiplications and summations include calculating loss , calculating gradients , and updating weights ? thanks ! answer : does loss compuation involve calculating any gradients ?",
    "33": "questions : does cross-entropy loss adds more weights ( 10 ) apart from ( over and above ) weights defined in cnn hi sir ( s ) , i am confused about if there are more trainable weights from the cross-entropy loss or not . can you tell which scenario is correct ? scenario 1 : assumed that the output of final linear activation layer is passed used for calculating the loss . in this case , we do n't have any more learnable parameters other than what is defined in the neural networkscenario 2 : assumed that there are 10 more weights added for cross-entropy layeredit : i interpreted that thedocumentationat pytorch indicates there are internal weights , so i asssume scenario 2maybecorrect answer : the cross-entropy loss has a variation called weighted cross-entropy loss , where different classes have different weights . in the case of weighted cross-entropy loss , the weight of each class is determined by the entity that trains the model and thus not trainable . for example , an object detection model trained by metro company will put more weight on the class of person because they want to catch people running around on trails.for hw2 , consider all classes have equal weights . also , by default , the cross-entropy function assigns the same weight to all classes ; please see `` weight=none '' .",
    "34": "questions : memory requirements does memory requirements in question 2 refer to amount of ram needed ? answer : yes . but we want you to calculate ram usage by yourself without looking at the machine status .",
    "35": "questions : meaning of ( 4,4 ) -center region hi class , in the first question in hw2 , we are asked to create a 10x10 image with `` ( 4,4 ) -center region '' set to 1.does it mean setting one pixel ( 4,4 ) or a region , say image [ 3:7 , 3:7 ] ( the center 4x4 area in the whole image ) to 1 ? thanks . answer : yes , the center 4x4 area in the whole image .",
    "36": "questions : reminder : our lecture is in person today ! answer : no answer",
    "37": "questions : about conv2d backward # of operations a trick you can use to figure out the # of operations in conv2d backward is to convert them into toeplitz matrice . since you know the forward and backward operations for matmul , then you know the # of operations for conv2d forward and backward pass . answer : no answer",
    "38": "questions : lecture5.pdf ( week 5 , 1 , arash ) has been added to class resources page under lecture notes the teaching staff has posted a new lecture notes resource.title : lecture5.pdf ( week 5 , 1 , arash ) https : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lmjaxsybpjp3euyou can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "39": "questions : pop quiz hi everyone , there `` might '' be a pop quiz on friday reviewing the material we have currently covered.arash answer : no answer",
    "40": "questions : hw2 ddl extension hi class , hw2 deadline has been extended to sep 17th . answer : no answer",
    "41": "questions : question about removing maxpooling2d i am wondering whether there are many different solutions to removing maxpooling2d so long as every layers ' input and output can match . and each solution corresponds to a different model accuracy . cause through discussion , i find classmates got different model accuracy for removing max pooling . or there is just one certain solution . answer : randomness in weights and different model architectures will result in different accuracies .",
    "42": "questions : reading assignment 1 & homework 3 assigned hi class , reading assignment 1 and homework 3 have been released . please check piazza resources - > homework.submit reading assignments on github as well . answer : no answer",
    "43": "questions : homework 1 grades and solution released . hi class , we have released hw1 grades and its solution . grades are on blackboard , and the solution is in piazza- > resources - > homework solutions.hw1 total points are 63. answer : no answer",
    "44": "questions : reading assignment 1 ddl ? i find that in gradescope the ddl is 9.22 , but in piazza it is 9.27. answer : please follow the ddl on piazza and github , which is 9.27",
    "45": "questions : lecture6.pdf ( week 5 , 2 , arash ) has been added to class resources page under lecture notes the teaching staff has posted a new lecture notes resource.title : lecture6.pdf ( week 5 , 2 , arash ) https : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lmtcm3vjmeg2bsyou can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "46": "questions : quiz 1 solution posted under resource- > homework solution answer : no answer",
    "47": "questions : are tpus closely tied to specific dnn architecture or are dnn model-agnostic hi sir , is the tpu v2/v3 closely tied to cnn architecture ? my meaning is that ( maybe ) in the future , we move towards better models for image tasks like transformers . can the existing architecture be used efficiently ( by efficiently i mean that we know that even cpu can be used to train ml models inefficiently ) ? thank you answer : according to my understanding , tpu is a matmul accelerator that is not specifically designed for convolution operation like eyeriss , so tpu can deal with more general models other than cnn . depending on your purpose ( inference or training ) , application ( vision or nlp ) , platform ( edge or cloud ) , etc. , you have different design options and you choose the best one for your accelerator . like lei said , tpu is a matmul accelerator and from my understanding of transformers architecture , the bulk of computations are matmuls so a tpu will definitely be better than a cpu . since you mentioned future models , the next generation of models will likely have more compute heavy activation and optimizer functions . although there is research going into running optimizers on gemms , these will likely run on the vector processor which could become the bottleneck if they are not powerful enough .",
    "48": "questions : 09/13 slide deck not uploaded hi sir , the slide deck for the lecture dated 09/13 is not listed . i checked other slide decks and could n't locate the slides.09/13 included how weight-stationary , nvdla , output-stationary are actually implemented.these slides were part of original lecture 4 slides , but may not have been posted . posted lecture 4 has 64 slides . ( and not 114 slides ) thank you answer : the current posted lecture 4 has 114 slides . can you download again ? https : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lmdx2oepsz37nz",
    "49": "questions : lecture7.pdf ( week 6 , arash ) has been added to class resources page under lecture notes the teaching staff has posted a new lecture notes resource.title : lecture7.pdf ( week 6 , arash ) https : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lmyyxqo4d8546iyou can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "50": "questions : accessing the folder ml_systems_hw3 hi all , can anyone tell where to find the above folder ml_systems_hw3 in github ? answer : please go to your homework repo and click the code button to download the zip file .",
    "51": "questions : reading assignment 1 is not opening in colab when trying to open colab to write the answers of reading assignment 1. i am getting this message : answer : you can click ok and navigate to colab . manually paste your github homework repo link to the box :",
    "52": "questions : reading assignment 2 released . hi class , reading assignment 2 has been released . please check piazza resources - > homework.submit reading assignments on github as well . answer : no answer",
    "53": "questions : 'align_val_t ' in 'utils.h ' when i want to test my code locally ( vs code + wsl + gcc ) ，it always reports : namespace \\ '' std\\ '' has no member \\ '' align_val_t\\ '' . how to solve the problem ? answer : did you use the exact compile command we provided in the script ? g++ -std=c++20 `` $ source '' -o build/ '' $ output_name '' -mavx -lblas -fopenmp change `` $ source '' to your source code filename and build/ '' $ output_name '' to your output filename we need to set c++20 standard .",
    "54": "questions : course start time at 4pm today dear studentstoday i will be teaching the ee599 class starting at 4pm and the discussion session is swapped to 6+pm . we will meet in sos b4 at 4pm . see you all in a few mins.thank youmurali answer : no answer",
    "55": "questions : not sure question in hw3 i am not sure in question2 hw3 , what is the meaning of `` reformat toeplitz y into row major '' ? which matrices is toeplitz y ? answer : if you apply matmul to your input toeplitz matrix and kernel toeplitz matrix , then your output matrix will be the toeplitz y. think about the row and column dimensions of this `` toeplitz y '' . is this `` toeplitz y '' stored in row-major ? ( i.e. , store the first row from the first channel , then the second row from the first channel , and so on . after going through the first channel , store the first row from the second channel , and so on ) . if not , we reformat it into row-major , as it will become the input for the next layer .",
    "56": "questions : paper 1 discussion hi sir ( s ) , can we have a group discussion ( like 30 minutes ) maybe after the discussion or during the discussion timings on the eyeriss paper .. or do you suggest that we can discuss among ourselves post the deadline.thank you answer : you are welcome to discuss papers with your classmates . in terms of scheduling time for paper discussion , please do so on your own . you are also welcome to post `` looking for study mates/group '' on the piazza .",
    "57": "questions : week 4 discussion recording missing on piazza week 4 discussion recording missing on piazza . kindly upload as soon as possible . answer : usually , we will post discussion recordings 1 day after the actual discussion . unfortunately , my recording file for week 4 is corrupted and no longer functioning .",
    "58": "questions : deadlines for ra2 and hw3 extended ! hi all , we have extended the ddl for ra2 and hw3.thank you , yongqin wang answer : no answer",
    "59": "questions : check results for hw3 what is a good way to check out outputs to be correct for hw3 ? does it have to do with the sum of all outputs ? i am assuming they should be the same . answer : right , the sum of all outputs is meant for verifying the results . you should get the same sum for all questions .",
    "60": "questions : late start for ee559 class tomorrow . start time will be 9:30am dear studentsi am going to be late tomorrow getting to the class . let us plan to meet at 9:30am and i will make up the class time another day as needed . sorry for the short notice.murali answer : no answer",
    "61": "questions : segmentation fault hi , may i know what could be the reason for segmentation fault ? secondly , my answers are n't matching to the naive implementation and i 'm not sure which is correct ? thanks , answer : segfault usually results from you writing to a location you are not suppose to write . sums from all questions should be the same",
    "62": "questions : q1 do we need to change the cache size , if yes , we should update to the l1d cache only right ? in the file titledq1_conv2d_naive.cc// modify the cachesize to match your system configuration in google colab const unsigned int cachesize = 72 * 1024 * 1024 ; do we need to update the cache size .. because , in the google colab , ! lscpuingoogle colabrevealscaches ( sum of all ) : l1d : 32 kib ( 1 instance ) l1i : 32 kib ( 1 instance ) l2 : 256 kib ( 1 instance ) l3 : 55 mib ( 1 instance ) so should i update this cachesize to 32 * 1024 same as l1d cache ? answer : yes , you need to change cache size to match your system.l3",
    "63": "questions : question for the materials for the last two lectures seems that some content in the last two lecture（especially the lecture in friday） i ca n't understand even after looking to the recording for many times . and i ca n't find any related information on the recommended book . i wonder is there any material recommended for me to understand better ? answer : in the coming discussion section , we will revisit model/data parallelism and coded computing for straggler mitigation .",
    "64": "questions : distributed ml slides and reading list of papers posted dear studentsi posted the slides on distributed ml i covered last week ( and a few more slides that i will cover next week ) . i have also posted 6 research papers from which i extracted the materials for the presentation . please review all of these materials.you can view all these files on the course page : /usc/fall2023/ee599/resourcesmurali answer : no answer",
    "65": "questions : hw3 should we be transposing the input ? for hw3 q3 and q4 , it tell us to reorder the weights into column major and write a matmul to transb . since performing convolution with toeplitz matrices is filter_weight x input , the input would be `` b '' so would n't we want to put the input in column major order and transpose it for the matmul ? in turn , we would keep the weights in row major order . answer : in the homework , we assume input toeplitz times kernel toeplitz , so for q3 and q4 , we store kernel toeplitz in column major .",
    "66": "questions : hw3 toeplitz into row major for q2-q6 , could you go into more detail in what you want us to do for the line 150 todo from this screenshot ? do you want us to transform the output of the toeplitz matmul back into the original convolution format ? answer : please check this post @ 73",
    "67": "questions : unable to setup the c++ vs code environment i am using vs code + wsl as suggested . have below error : i tried installing compiler gcc using the below command.sudo apt-get install g++have below error : kindly help resolve these logistics issue . answer : can you compile a simple `` hello world '' c++ code in wsl ? what is your compile command ?",
    "68": "questions : important information about the midterm exam dear students , below are important information about the midterm exam : midterm will be onoct 11th from 4 to 6pm.the exam is * * in-person only * * in our usual class , sosb4.the exam is closed book , closed notes , using pen and papers.students are not allowed to use , wear , or access any electronic devices during the exam . this includes : cell phonesheadphonesearplugssmart wearablesthe exam covers all concepts that were discussed in our lectures so farexcept for the distributed ml lectures . this includes : dnn taxonomyfamous cnn and dnn modelsconvolution and cnnsperformance evaluation and roofline analysismatrix operationsdataflow and stationarinessquantizationtputhe reading assignment papers ( eyeriss and tpu ) there might be programming questions on the exam . in that case , you can write the answer either in python or pseudo code . you do not have to remember the exact syntax of python and pytorch functions.thanks , arash answer : no answer",
    "69": "questions : gradiveq slides presented in class , not found as part of slides posted hi prof. murali , gradiveq slides presented in class , not found as part of slides posted on the resources page of piazza. # of slides presented=94 # of slides in the posted pptx = 69also , i see some differences in a few of the slides , what was presented vs posted.eg : mds ( 4,2 ) coding slides , the linear combination was showed as equations on the slides presented in class ( a1 , a2 , a1+a2 , a1+2a2 ) but on the slides posted on piazza , it 's shown pictorially ( which is confusing ) .please do the needful . answer : i am not sure what the discrepancy you mentioned . i just reposted the slides . please re-download .",
    "70": "questions : grace period / late policy for homework due to an unforeseen event , i am finding it challenging to complete the assignment by the scheduled deadline . would it possible have grace periods or late policy for assignments ? thanks . answer : email me .",
    "71": "questions : quiz 1 grade released hi all , quiz 1 has been graded . if you do not receive a grade , please contact tas ( lei , yongqin ) .please pick up your answer sheet during office hours eeb 230.total possible points : 50ptsthanks , yongqin answer : no answer",
    "72": "questions : monday oct 9th 6pm-8pm extra review session dear all , we are planning an extra review session on oct 9th , 6 pm - 8 pm in eeb248 . during this session , chaoyi will go over some review questions for midterms . this session will be helpful for your mid-term . please attend this session ! thanks , yongqin wang answer : no answer",
    "73": "questions : invited speaker hi everyone , i would like to invite you to this invited talk by professor david patterson.i made sure that we schedule it during our regular class time slots so that everyone can attend.please save the date and i hope to see everyone.see theflyer here.arashhttps : //usc.zoom.us/j/98706499300passcode : 592504 answer : no answer",
    "74": "questions : tpu paper : raw dependency in the tpu . can you explain ? hi teaching staff , fellow students , in table 3 , page 7 , while discerning the reasons on why cnn1 did n't performed well on the tpu despite being computationally heavy , it was pointed out that that almost 23 % of the cycles are spent on raw dependency.i am unable to visualize/comprehend , what is the cause of raw dependency in the tpu.my conjecture ( that i want to be improved ) is that may be accumulators are storing partial results , and next computations require those results are waiting for partial results from the accumulators to be read and again processed.the paper attempts to explain the data flow in figure 4 , but i did n't understand.please share your reasons . thanks in advance . answer : they do not provide the full context , so we can just make some guesses.here are some examples where raw dependencies can occur during neural network computations on a tpu : matrix multiplication : when calculating z=x x y , we might have to wait for x and y to be computed first , if they are not already calculated.tiling : when a large operation is broken into tiles , calculating the final result has raw dependency on the tile results.backpropagation : if you 're updating a weight based on gradient values , the weight 's new value depends on its previous value.also , given that cnn1 has shallow features depths , some other possible reasons are : some mac units might be waiting for data ( reads ) from previous operations ( writes ) that involve a small subset of channels . as a result , many units remain idle , leading to inefficiencies.data movement overhead : with fewer feature maps , there might be more frequent switching between layers or operations in the cnn . this can lead to more frequent data movement between the tpu 's memory and its processing units , exacerbating raw dependencies . waiting for data to be written ( from a previous operation ) before it can be read ( for the next operation ) can introduce stalls.hope that helps.arash",
    "75": "questions : tpu : operations per byte needed to reach peak performance is 1350 .. how did they reached deduced in the paper size of accumulator was pickedup as 4096 after noting that operations per byte needed to reach peak performance was 1350.i am unable to arrive upon at how did they deduced 1350 from the peak performance .. any formulae or intuition or hint on how to arrive upon at the peak performance .. and then deduce this 1350 ? thank you a lot answer : peak performance is the maximum number of operations ( like macs ) the tpu can perform per second . this can be calculated based on the hardware and the total number of macs.example : in a tpu with 256x256 macs , assuming each mac executes two operations per clock cycle , ( add , mult ) , you can write:65,536×2=131,072 operations per cycleassuming a clock frequency of 1ghz:131,072 ops/cycle×1,000,000,000 cycles/second=131 ( tops ) of course this is the max theoretical performance . in fig 5 , this value is around 100 tops.memory bandwidth : this is the rate at which data can be read from or stored into the memory . also depends on the memory type.having these two values , you can calculate the operations per byte at the interception of the two lines ( fig 5 ) .",
    "76": "questions : homework 2 & 3 solution posted hi class , homework 2 and 3 solutions are posted on piazza . please check . for the input/weight stationary question . we will get back to you tomorrow . thank you , tas answer : no answer",
    "77": "questions : week 8 discussion recording has been uploaded the recording of today 's discussion session ( quiz 1 problems ) has been uploaded in the resources page . answer : no answer",
    "78": "questions : comments on weight/input stationary for 2d weight stationary , if we only use `` for loops '' ( without parallelization ) , using the notation defined in class , we want f [ m , c , r , s ] to stay stationary as much as possible . so `` m , c , r , s '' should be at the outmost loops . therefore , the implementation is likesimilarly , for input stationary , we want `` c , h , w '' are at the outmost loops . answer : no answer",
    "79": "questions : ta office hour location changed to phe320 hi all , due to unforeseen equipment arrival , eeb230 can no longer be used as an office . ~~ta office hours will be held at tables between eeb and rth . if you did not find your ta there , please fetch your ta in eeb224.~~ ta office hours will be held at phe320 . thanks , tas answer : no answer",
    "80": "questions : explicit-decoupled data orchestration : address space not necessarily to be answered before today 's exam .. when we say that scratchpad is like a explicit scheme , then do the addresses ( or address space ) on the scratch pad looks to the programmer as part of the memory addresses itself and is just implemented as scratchpad or does scratchpad has a different space like in cpus we had cpu i/o as memory i/o ? let me elaborate by what i mean by separate address space .. suppose i have 16 elements with each element being a byte as scratchpad memory .. so as a programmer ( since scratchpad is explicit coupled ) how do i address aka read/write to these scratchpad address .. does iris or any other accelerator implementing the scratchpad provides address range to be used or some base register so that we know the address to write into and read from ? answer : check the professors ' responses below .",
    "81": "questions : gpus vs iris/tpu architecture : is data marshelling a key difference sir ( s ) , can i say thatkey difference which i find between gpus and iris is thatgpus relies on multiple hardware warps ( kinda of what gandhi sir says as fine-grained multi-warping ) so that if one warp is stuck ( say this warp required load word and 32 accesses from 32 individual threads do n't align on the same cache line ) then gpu can switch to a different warp .. whereas , iris requires ( and for that matter even tpus ) alldata be marshelledat once to the accelerator so that we can pipeline the reads from the memory and it is the requirement of iris & tpu that data is aligned so that it can be read at once ... it is thismarshelling of data , which leads to advantage at the time of inference ( in tpu v1 ) and as a con , makes 's tpu 's ( or iris 's ) deployment specific to a task like nn accelerator .. whereas gpus are more robust , ( robustness comes from gpus placing no such demand to marshalling data ) allowing gpus to be have wider use cases .. however , both tpus ( and iris ) do provide a potent alternative if my organizations need are well-thought-out and meeting what tpus ( and iris ) are designed forso , when comparing the gpu vs iris performance , unless this marshelling activity is hidden , otherwise , time required for marshelling may be added to the iris/tpu answer : you are right . accelerators like tpus are designed for high-throughput , parallel processing of tensor operations . to make the most of this capability , data must be marshaled or transferred in large chunks to the tpu . loading data in bulk minimizes the overhead of frequent memory accesses and optimizes the bandwidth utilization.the ability to pipeline reads from memory is a critical feature for tpus . this ensures that while one chunk of data is being processed , the next chunk can be read from memory , and thus the tpu is kept busy.for the tpu ( or eyeriss ) to read data efficiently , the data needs to be appropriately aligned . misaligned data can lead to inefficient memory access patterns , reducing the performance benefits of the specialized hardware .",
    "82": "questions : office hours during fall recess will office hours remain the same during fall recess or will it be appointment-based ? answer : fall recess is a campus-wide holiday , so there will be no office hours .",
    "83": "questions : homework 4 posted ! https : //classroom.github.com/a/hrh7pkdu answer : no answer",
    "84": "questions : federated ml slides hi prof murali , could you please upload the slides related to the federated ml lectures ? answer : and some updated distributedml slides for review . thanks !",
    "85": "questions : final project posted & form your group ! the final project has been posted under resources homeworkhttps : //classroom.github.com/a/s_bvjwcz.please form your own group and start reading project descriptions as soon as possible.register your group information here by the end of next wednesday : https : //docs.google.com/spreadsheets/d/1otmidbxrlqiy6hjhn1jmim2xtoizb74y8xdlzerrwya/edit # gid=0thanks , yongqin answer : no answer",
    "86": "questions : looking for a teammate for the llm project any buddy interested in project 1 and don ’ t have a teammate yet ? please contact me if you are interested : ) answer : no answer",
    "87": "questions : virtual memory concepts for secure enclaves dear students , yesterday we discussed the privacy concerns with federated learning . i mentioned that there exists techniques such as differential privacy , multi party computing and secure enclaves to protect against privacy leaks . i will cover multi party computing in class tomorrow and i * hope * to cover enclaves in class after differential privacy ( as time permits ) . but to understand secure enclaves a deeper understanding of virtual memory is a must . i hope you all are familiar with virtual memory in your prior course work . if not , you are welcome to attend the virtual memory lecture i plan to start today in my ee557 class at 4pm in ohe 132. it is just a coincidence that the timing worked out . you are welcome to attend 557 lecture so as to familiarize yourself with virtual memory . that way when i discuss secure enclaves it is a bit easier to comprehend that enclave notion later.murali answer : no answer",
    "88": "questions : question about final project do we need to submit the answer for the final project 's pdf ? or just understanding is okay answer : you will need to understand and write down the answer for each of the questions asked in the project .",
    "89": "questions : midterm average could i ask what the average for the midterm was ? thank you . answer : we will collect the results and release the statistics tomorrow .",
    "90": "questions : selection of final project could we select second project if we finish the first one before the due to get bonus grades ? answer : @ murali annavaram @ arash",
    "91": "questions : ask for ppts of this week hello professor murali , if you have free time , could you please upload ppts of this week . i am afraid i have something not fully understand and need to see some details.thanks ! answer : professor murali has uploaded the material .",
    "92": "questions : looking for a teammate for the llm project hi i 'm gaurav , i am looking for a teammate for the llm project . please contact me at gkudva @ usc.edu .thanks , gaurav answer : no answer",
    "93": "questions : midterm grades released ! we have released your midterm grades on blackboard.average is 78.8 , median is 81 , std is 9.7.we will give you all midterms in the end of wednesday discussion . regrading period is open till nov 3rd in-person office hour only.thanks , yongqin answer : no answer",
    "94": "questions : looking for a teammate for the private ml project hi all , i 'm haochen wu . i am looking for a teammate for the private ml project . you can reach me at wuhaoche @ usc.edu if you are interested . thanks ! answer : no answer",
    "95": "questions : clarification for questions on hw4 for these questions on hw4 , do you want the answer in code form implementation ? or should we write out the formulas in text ? answer : we are asking for the formula , which is then needed in your implementation . so first write the formula .",
    "96": "questions : final project decision before 10/25 hi class , please decide which project you want to work on by this week 's discussion session on wednesday , oct 25th . we will go through each of the project descriptions . thank you , tas answer : no answer",
    "97": "questions : hw4 : deepcopy what 's the purpose of copying input and output activations hi , there is a function ( that we do n't need to modify ) with the name copy_model . i understand that we need to do deep_copy and also need to copy the scale .. what 's the purpose of copying input activations and output activations ? and follow-up , why does the model saves input and output activations . most likely , my understanding is not correct , about what we mean by input activation and output activation ? my meaning is that there can be 10,000 images ( and thus 10,000 ) input and output activations .. why do we need to save these ? thank you answer : this should become very clear as you go through the rest of the assignment . we store activation so that we can use them to find the scaling factors . have you gone through the rest of the assignment yet ?",
    "98": "questions : lecture slides for distributed and federated learning hi professor , could you please upload the federated ml and updated distributed ml slides ? it would help us while studying these topics . answer : check professor murali 's response below .",
    "99": "questions : ( optional ) theoretical question : request for intuition-based arguments the book while discussing reducing precision ( section 7.1 , page 149 , search “ reducing the bit width of the fully… …possible partial sums ” ) makes following points : reducing the bit width of the fully accumulated partial sum willnot have a significant impact on accuracy if the distribution of the weights and activations are centered near zero with a limited standard deviationthey reach this conclusion ( reduced bit widths ) by observing that near-zero centered distribution and limited standard deviation has lower range , so lesser widths can be used ? since the bit width of the final output activation will ultimately be less than the maximum possible partial sum , why they say that bit width of final output activation will ultimately be less than max . possible partial sumi know these questions are more theoretical ( and my lack of mathematics background is a hindrance , ) so , is it possible to give some intuition ( no worries if not possible . ) answer : 1. if the weights and activations are mostly centered around zero , it means most of the values are small , specially when standard dev is small . as a result , they can be represented with fewer bits without losing much information.2 . remember that after accumulating the partial sums , the result passes through an activation function . for example , the sigmoid or the relu function . they potentially reduce the number of bits .",
    "100": "questions : no item provided answer : no answer",
    "101": "questions : midterm solution posted dear all , we have posted the midterm solution under resources . you can only request regrading during the in-person office hours . the regrading window is open til nov 3rd.thanks , yongqin answer : no answer",
    "102": "questions : recommendation-models lecture notes dear alli posted the slides i covered yesterday on recommendation models , along with some additional slides that i did not cover but i thought it will be helpful if you want to read on your own . these slides are from the text book `` recommender systems : an introduction '' textbook which i believe has e-book version . most of the slides are from that textbook but some additional slides were added for comprehensive coverage . you are only responsible for learning what we discussed in the class : collaborative filtering schemes , content based filtering schemes and deep learning recommendation models which is yet to be covered.title : recommendation-models-ee599.pptxhttps : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lo7q4q2ifuz2royou can view it on the course page : /usc/fall2023/ee599/resourcesmurali answer : no answer",
    "103": "questions : carc tutorial posted hi class , carc tutorial slides and video are posted under piazza resources . please remember to terminate your session when you are not running any work to save computation units.you can check how many units you have used so far from carc user portal : https : //hpcaccount.usc.edu/thank you , tas answer : no answer",
    "104": "questions : in problem 3 of hw4 : 4-bits : do the weights also need to be updated to 4-bits instead of 8-bit integers , modify your code to use 4-bit integers . recalculate the accuracy.in this , do we need to change the weights scale as well ? answer : yes . please report the accuracy for both weights and activations quantization in 4-bit .",
    "105": "questions : due time for hw4 i see the due time and find it is tomorrow morning . should it be oct 30 , 2023,23:58 pdt ? or it is set by this time , thanks . answer : updated .",
    "106": "questions : homework 5 posted title : homework 5https : //classroom.github.com/a/sszvi9tgdue : nov 10th answer : no answer",
    "107": "questions : regrading midterm q4 hi all , after discussion with the whole teaching team , we decided if your solution to q4 is similar to the new posted additionalsolution . we will give you full credits.please submit your regrade request during the ta office hour by the end of nov 3rd.thanks , yongqin answer : no answer",
    "108": "questions : can not run llm by following the quickstart hi , i have meet some issue when run the llm by strictly follow the quickstart guide in llama repo . every time the program stuck after this command : singularity > torchrun -- nproc_per_node 1 example_text_completion.py -- ckpt_dir ./llama-2-7b -- tokenizer_path . -- max_seq_len 32 -- max_batch_size 1 > initializing model parallel with size 1 > initializing ddp with size 1 > initializing pipeline with size 1 ( stuck here , gpu usage is almost 0 ) i also try this command still met the same issue : singularity > python -m torch.distributed.launch -- nproc_per_node=1 example_text_completion.py -- ckpt_dir ./llama-2-7b -- tokenizer_path . -- max_seq_len 32 -- max_batch_size 1 thank you for your help ! answer : i think i have fixed the issue by change the default device try to increase the number of cores when you request resources . i am using 8 cores , and it takes 50 seconds to load the model and generate the results . here is the command i am using : `` ` python -m torch.distributed.run -- nproc_per_node 1 example_text_completion.py -- ckpt_dir /project/saifhash_1190/llama2-7b -- tokenizer_path /project/saifhash_1190/llama2-7b/tokenizer.model `` ` can i make your post visible to the class ? i think that would be helpful to others .",
    "109": "questions : chipnemo : domain-adapted llms for chip design https : //d1qx31qr3h6wln.cloudfront.net/publications/chipnemo % 20 % 2824 % 29.pdf found this paper that tries to adapt llm for chip design .. in line with another paper this year chateda ( although only 2 citations ) : that showcases how synthesis constraints can be written in a chat interface and script be generated . ps : i have n't read any of these , just found the concept at the intersection of dnns and chip design answer : no answer",
    "110": "questions : project milestone submission is open . submit your milestone by nov 5th.https : //classroom.github.com/a/scqbag3k answer : no answer",
    "111": "questions : slides for tomorrow posted dear students.i posted slides relating to what i plan to cover tomorrow , which relates to fault tolerance in dnns.murali answer : no answer",
    "112": "questions : class late start at 9:30 am on nov 3rd dear students , let us start the class tomorrow at 9:30am ( half hour later ) . hope the later start will make it easier for all to show up in class on time.murali answer : no answer",
    "113": "questions : postulate : why we shuffle the sampels forming a batch at each epoch hi teaching team , during the discussion related to why we haverandomimages ( considering cnns ) form part of a batch and shuffling this each time we begin a new epoch.can i say that this randomization ( hand-waiving argument is that ) allows robustness because these days networks employ batch normalization ( bn ) and at each layer ( where bn is applied ) activations are subtracted and divided by mean and std . dev.for that batch ( during training ) so , having different set of images , randomizes ( kinda of noise ) and helps makes the network more robust.in earlier networks ( before bn in 2015 , before resent and googlenet ) we used to add dropout layers to make it robustness . ( and achieve similar purpose as is being done by bn ) thank you answer : anotheronline postin supplementary to prof. murali ’ s response .",
    "114": "questions : hw5 implementation tips hi class , from q1 , we know that function loss.backward ( ) computes gradients for each of the parameters in the model based on forward propagation , optimizer.step ( ) applies the gradients to update the parameters , and optimizer.zero\\_grad ( ) clears the gradients . in order to access gradients of the parameters in the model , please do this : `` ` for param in model.parameters ( ) : print ( param.grad ) `` ` param.grad is initially set as none and becomes a tensor the first time a call to backward ( ) . for q2 and q3 , you need to manipulate the gradients based on the method shown above , and should get the same training results as q1 . thank you , tas answer : no answer",
    "115": "questions : backprop and systolic arrays found this article that shows how backprop can be mapped on systolic arrays.backprop and systolic arrays .. i ’ ve had a serendipitous encounter on… | by yaroslav bulatov | mediumcompletes our discussion on jacobian matrices during initial lectures answer : no answer",
    "116": "questions : hw5 colab environment issue hi class , we found out that due to out of memory ( > 12gb ) issue on colab , you would encounter the following error when you execute q3 and q4 : `` ` error message : primary job terminated normally , but 1 process returned a non-zero exit code . mpirun noticed that process rank 4 with pid 0 on node ... exited on signal 9 ( killed ) `` ` to fix this issue , let 's reduce the number of ranks for each question . for q2 , set rank size = 2 , such that : ` mpirun -n 2 ` for q3/q4/q5 , set rank size = 3 , such that : ` mpirun -n 3 ` answer : no answer",
    "117": "questions : q & a with david patterson and distinguish lecture hi everyone , as a reminder , professor david patterson will be on campus today : his main talk is at4:00pm - 5:00pmineeb 132.i would like to see everyone attending this session . there will be no lectures today.there is a q & a session with him for our class today at2:30 to 3pmineeb 539. we have a very limited capacity , so please let me know under this post if you are attending . if so , please make sure you carefully select a technical question that benefits other students as well.there is a reception from3 to 3:45pm ineeb courtyard.arash answer : no answer",
    "118": "questions : question about files in carc may i know what are the files 'consolidated.00.path ' and 'tokenizer.model ' ? does the file 'tokenizer.model ' contain 'model.py ' and tokenizer.py ' ? is the file 'consolidated.00.path ' a dataset ? because i did n't find any illustration of in llama github respository . answer : in order to run the inference , you can refer to the github repo main page or this post : @ 135 then , you can track how the command arguments are processed by the code . for example , the argument ` ckpt_dir ` is eventually taken by this line of code : https : //github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/generation.py # l101c39-l101c39 . it implies the ` consolidated.00.pth ` file is the model weights . the model is loaded by https : //github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/generation.py # l107c11-l107c11 for the other two files , as part of the learning process , please track and understand the code carefully before you make modifications accordingly .",
    "119": "questions : recording of the distinguished guest lecture hi , i saw that the session was recorded on zoom . since a lot of ground was covered in a short span , i wanted to listen to it again . where can we find this recording ? answer : i am not sure where to find the recording , but there is another similar speech onyoutubeand here is thepaperprof . david presented .",
    "120": "questions : hw5 extended to monday hw5 extended to monday . answer : no answer",
    "121": "questions : introduction to transformers ( for students working on the llm project ) hello everyone , i recorded this lecture ( an introduction to transformers ) for those of you who are working on the llm project.i will be teaching transformers next week in the class in more details , so you do n't have to watch this video unless you are working on the llm project and need to have a background knowledge on language models and transformers.here is the link to the video : https : //youtu.be/9gdzn1qxlw8you can view it on the course page : /usc/fall2023/ee599/resourcesarash answer : no answer",
    "122": "questions : code modulation doubt hi lie , i did the modifications and it 's in comment form . please could it be reviewed ? i feel the cache_v and cache_k is necessary since it needs to store intermediate activations within the gpu.to test this , should i use the same command present in # 135 post ? answer : when replacing fairscale nn modules with pytorch nn modules , you do n't need to initialize weights . ( you will need to load the pretrained llama weights into it later ) you can remove kv caching as it is meant for inference speedup , which is good to have but not a must . in addition , since we will use llama model for training later , this feature must be disabled.you can test your code again with the original command to run some inference because you are not supposed to change the model architecture , and you can still load the original weights .",
    "123": "questions : homework 6 posted homework 6 posted.due : dec 1st , 2023 answer : no answer",
    "124": "questions : requesting to make hw6 optional hi professor , we totally get that diving deep into the subject is crucial for us students.lately , things have been pretty intense , especially with the final project taking up a substantial chunk of time . balancing that alongside tasks from other classes feels like juggling , honestly.by any chance hw6 could be made optional ? it might ease off a bit of pressure and give some breathing room to focus on the project and other coursework.i reckon a bunch of other students would support this idea . if you 're on board or if others agree or have other suggestions , maybe drop a reply below to gauge the general sentiment ? appreciate your understanding ! answer : i agree , we have another project to complete for ee-577b . it would be great if hw6 could be optional and be graded for extra credit .",
    "125": "questions : submit your milestone 2 by this sunday https : //classroom.github.com/a/f7teqgwv answer : no answer",
    "126": "questions : regarding llm kv cache hi lei , could you send the url which showed before today 's discussion ? there is gif to show the compute process ( attention ) .i delete the codes related kv and fairscale ( distributed training ) , but it generate text with no sense . i want to check the compute process , especially about mask.thank you ! answer : kv cache explanation : https : //medium.com/ @ joaolages/kv-caching-explained-276520203249also there is a good video on this : https : //www.youtube.com/watch ? v=mn_9w1ncflo",
    "127": "questions : hw6 is now for extra credit dear all , due to the final project 's difficulties , we now make hw6 optional , and you will be able to earn extra credits for completing hw6.thanks , yongqin answer : no answer",
    "128": "questions : office hours today changed to at 12pm-1:30pm pacific time dear alldue to a conflicting meeting i have to move my scheduled office hours to 12pm-1:30pm , instead of 11am-12:30pm . please make note . for anyone who needs to use zoom here is the zoom linkhttps : //usc.zoom.us/j/2324883543murali answer : no answer",
    "129": "questions : introtonlp.pdf has been added to class resources page under lecture notes the teaching staff has posted a new lecture notes resource.title : introtonlp.pdfhttps : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lp2vfue83oq77wyou can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "130": "questions : hw6 correction for those who are interested in hw6 , we need to make a correction : for q1 : change sigma = 0.5for q2 : run dp-sgd with sigma = 0.8 again.sorry about the confusion . answer : no answer",
    "131": "questions : llama2-7b parameter breakdown by layers llama2-7b parameter breakdownmaybe incorrect , but gives a general overviewi am wondering why so much weightage given to mlp blocks , whereas in cnns , we saw the we generally over the generations of cnns shifted weights towards cnns vis a vis fully connected layers .. there are 32 such transformer blocks in llama2-7binspired from https : //medium.com/ @ saratbhargava/mastering-llama-math-part-1-a-step-by-step-guide-to-counting-parameters-in-llama-2-b3d73bc3ae31 # : ~ : text=llama % 2d2 % 20uses % 20the % 20bytepair , an % 20embedding % 20dimension % 20of % 205120 . answer : no answer",
    "132": "questions : in llm project , why we do n't need kv cache during the training hi teaching team , i remember that llama also works on next token prediction . so , why we do n't make use of kv cache in training as well ? thank you answer : you should first understand the functionality of kv cache .",
    "133": "questions : llm project suggestions hi class , i created arepoof simplified llama inference for your reference . in inference.py , i have removed fairscale ( for distributed inference ) , fire ( for command line arguments passing ) , kv-cache ( for inference speedup ) , and distributed launch ( required by fairscale ) .i also simplified the alpaca training code in finetune.py . you can run it to check the data pre-process results , and build on top of this code to implement the training loop . in terms of loss computation , please refer to the hf libraryhere.you can instantiate a llama model with only one decoder layer ( without loading weights ) to test your code when implementing . after it is bug free , you can expand the llama model to full size and load pre-trained weights to train it on a100/v100 gpu.thank you , lei answer : no answer",
    "134": "questions : [ important ! ] : final exam date hi everyone , as we have announced earlier in the semester , the final exam is ondec 8 from 2pm to 4pm , and is an in-person exam.important : if this time conflicts with exams from any other courses that you are taking , please let me know asap . if i do n't receive any notes in this regardby this friday , i will assume this time slot works for everyone.note that we canonlyaddress other course examconflicts . no other reasons , such as traveling , etc , is acceptable.thanks , arash answer : no answer",
    "135": "questions : [ important ! ] : final exam date survey hi everyone , we need to make sure the announced final exam date works for everyone and does not overlap with final exams from any other course.please fill out this form asap with your answer : https : //forms.gle/qxjd689cmbc5tqlz6note : answer `` no '' only if this time overlaps with another exam.thanks , arash answer : no answer",
    "136": "questions : final project deadline extended please check your github classroom . we extended the ddl to dec 8th . answer : no answer",
    "137": "questions : llm project update hi class , i have refactored therepo , and the naive training loop is provided in finetune.py . please pull it again and take the current code base as a reference . ( you do n't have to design your code structure like this ) for the rest of the project , please implement lora , gradient accumulation , gradient checkpointing , and mixed precision.i will be hosting office hours next week on monday , wednesday , and friday , at the same time slot as before.lei answer : no answer",
    "138": "questions : lecture schedule this week hi class , the last lecture will be on this wednesday 6:30-8:20pm.there will be no discussion on wednesday . friday is also no lecture.please come to office hours this wednesday 1-3pm and friday 3-5pm if you need help with the final project . answer : no answer",
    "139": "questions : question about running finetune.py screenshot_2023-11-28_at_1.50.45_pm.pngconsidering the 'consolidated.00.pth ' is more than 10g , is that normal when i run python finetune.py . and show 'killed ' on usc carc ? answer : problem solved !",
    "140": "questions : transformers.pdf has been added to class resources page under lecture notes the teaching staff has posted a new lecture notes resource.title : transformers.pdfhttps : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lpk1e64giik1i3you can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "141": "questions : today 's class will be in-person dear students , today 's class will be in-person . please ignore my previous note.see everyone in the class today ! thanks , arash answer : no answer",
    "142": "questions : this article talks about the arithmetic intensity for various layers of dnns https : //docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html # gpu-arch may be useful for interviews answer : no answer",
    "143": "questions : reminder : there will be an in-person lecture tomorrow hi everyone , as i mentioned in the last lecture , tomorrow will be our last lecture . hopefully , i will see everyone in the class.arash answer : no answer",
    "144": "questions : transformers_training.pdf has been added to class resources page under lecture notes the teaching staff has posted a new lecture notes resource.title : transformers_training.pdfhttps : //piazza.com/class_profile/get_resource/llcjbpznzvv7c5/lpmzcjvegb45keyou can view it on the course page : /usc/fall2023/ee599/resources answer : no answer",
    "145": "questions : will we get some sample problems for final exam ? will we get some sample problems for final exam ? answer : no answer",
    "146": "questions : question about hw4-6 solutions may i ask for the solutions for hw4 , hw5 and hw6 ? answer : hw4-6 solutions are posted under resources . please check .",
    "147": "questions : model parallel clarification hello professor , in model parallel once we divide the layers of model into separate gpu 's , there should be a connection between gpu , so that intermediate activations of first set of layers in first gpu is sent as input to next set of layers in the second gpu , but in the slides describing model parallel it shows connection of separate gpu 's to master , is n't this data parallel . can you please clarify on this ? answer : no answer",
    "148": "questions : office hours this week hi class , there will be office hours today 4-6pm and wednesday 1-3pm . answer : no answer",
    "149": "questions : final project demo shedule hi class , please fill out thisformto schedule your final project demo . each group will have 10 minutes . during the demo , we will check the following : whether the code is executableask for explanations about the code structureask for understanding of the techniques and corresponding implementation details answer : no answer",
    "150": "questions : llm project loss i want to check if such loss change is right ? the loss at every begin of the epoch will larger than the previous one , but the whole trend is decreasing.is this a normal fluctuation caused by a limited number（total 200+ samples , print average loss for every 50 samples）of particularly special samples ? answer : it is possible because of the way you average or the way you print out the loss . as long as the loss is decreasing , you should be fine .",
    "151": "questions : final project report hi , when and how are we supposed to submit the project report ? also , are there any guidelines on how the report is supposed to be ? thank you answer : please use thislinkto submit your final project , the due date is dec 8th.for the report , please do the following : show training loss of your code.show some examples of prompts and responses before and after fine-tuning ( some prompts should be from the training set , and some prompts should be outside of training sets ) .report trainable parameter count and percentage after enabling peft , and report lora rank , alpha , and drop-out values.report where you placed the gradient checkpoints.report memory usage after applying each of the techniques.comprehensive analysis of each of the techniques ( follow the project description ) .",
    "152": "questions : important information about the final exam important information about the final examdear students , below are important information about the midterm exam : midterm will be on dec 8th from 2 to 4pm in eeb 132.the exam is * * in-person only * * . you must attend the exam.there will be no compensation exam.the exam is closed book , closed notes , using pen and papers.students are not allowed to use , wear , or access any electronic devices during the exam . this includes : cell phonesheadphonesearplugssmart wearablesthe exam covers the following topics : before the midterm ( there will be less focus on this part ) : dnn taxonomyfamous cnn and dnn modelsconvolution and cnnsperformance evaluation and roofline analysismatrix operationsdataflow and stationarinessquantizationtputhe reading assignment papers ( eyeriss and tpu ) after the midterm ( the main focus of the exam ) : distributed mldata/model parallelparameter server/decentralized trainingcommunication primitivescoded computingmulti-party computingfederated learningdifferential privacydp-sgdfl with local dpfl with central dprecommendation modelscollaborative filteringcontent-based filteringembedding tables and dlrmlanguage modelsdifferent models introduced in the classrnnlstmseq2seq encoder/decoder architectureattention mechanismembeddingstransformersarchitecturepositional encodingcosine similarityself-attention mechanismmulti-head attentiontraining of transformersevaluationthere might be programming questions on the exam . in that case , you can write the answer either in python or pseudo code . you do not have to remember the exact syntax of python and pytorch functions . answer : no answer",
    "153": "questions : embedding tables and dlrm hi professor , did we ever go over embedding tables and dlrm in class ? if so , is there a lecture recording ? i do not remember going over the slides.thanks answer : there are a few slides ( around page 96 to 122 ) from the rm lecture that cover these two topics . for embedding tables and dlrm , you are only responsible for what appears on the slides .",
    "154": "questions : please release carc machines for demo . if a team has completed their demonstration or is in possession of multiple machines , kindly release them . unfortunately , we are currently unable to secure a40 , a100 , or p100 machines for the demo . answer : no answer",
    "155": "questions : final exam grades have been released dear all , we have released your final exam grades . you can check them in blackboard . statistics of final exam is the following : max : 94min : 18median : 68standard deviation : 16.6thanks , yongqin wang answer : no answer",
    "156": "questions : grading has completed hi class , please make sure that each of your assignments , homework , and exams has a grade assigned to it on blackboard.thank you for your dedication and hard work throughout the semester . have a good winter break ! the teaching team answer : no answer",
    "157": "questions : final letter grades are published dear students , your final letter grades are now published on oasis . here are some explanations about your grades : we used the tables below ( also published in the syllabus ) for your grade calculation , however , we incorporated a very generous method for giving everyone up to 18 % total extra credit as follows:5 % for each reading assignment ( total of 10 % ) .8 % for hw6.this means we first calculated your final grade using table 1 , ( excluding reading assignments 1 , 2 and hw6 ) , then we added up to 18 % based on your grades on the reading assignments 1 , 2 and hw6.the extra credit on the final exam was applied to your final exam grade itself.your final letter grade is calculated based on table2.please note that due to the rather generous extra credit points , there will be no curving or any other special considerations.if you have any questions or concerns about your grade , please contact us asap ( by the end of today ) .thanks and best of luck , arash and murali answer : no answer"
}