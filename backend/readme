# e_TA backend
## File sturcture

backend/
├── README.md
├── setup.py 
├── eTA/
│   │
│   ├── app.py                  # main file, run this to start server
│   │
│   ├── env/                    # environment setting 
│   │   ├──          
│   │   ├── requirements.txt 
│   │   └── ...              
│   │
│   ├── controller/
│   │   ├── __init__.py           
│   │   └── handleQuery.py       # Getting response from embedding and LLM 
│   │  
│   └── service/    
│       ├── embedding/           # embedding model 
│       │   ├── __init__.py  
│       │   ├── src/             # data needed for embedding model 
│       │   ├── getAnswer.py     # get similar Q&A from database
│       │   └── voc2Doc.py       # model training
│       │
│       └── LLM/                 # LLM API
│            ├── __init__.py
│            ├── gpt.py          # call GPT 
│            └── llama2.py       # call llama2
│    
│                                
└── README.md
# Devops
## Run conda envirment before run the program
```
conda activate DirRes #Mario's env
```
## Running backend program on the backend(always run)
```
nohup python3 app.py > output.log 2>&1 &
```
## Check the PID of the program occupying the certain port
```
lsof -i :PORT
```

